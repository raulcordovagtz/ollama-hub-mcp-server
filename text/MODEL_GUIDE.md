# 游 Gu칤a Maestra de Texto y Visi칩n (Ollama Infrastructure)

Este documento detalla la operaci칩n del servidor de inferencia ling칲칤stica y visual (VLM).

---

## 游 Perfiles de Modelos

### 1. `qwen3:8b` (5.2 GB)

**Especialidad:** Razonamiento balanceado y Creatividad.

* **Perfil:** El "Intelectual 츼gil".
* **Fortalezas:** Excelente en espa침ol, responde r치pido y consume pocos recursos. Ideal para res칰menes, chats generales y poes칤a.
* **Contexto:** Cargado por defecto para herramientas de chat MCP.

### 2. `qwen3-vl:32b` (20 GB)

**Especialidad:** An치lisis Visual Profundo (VLM).

* **Perfil:** El "Ojo Cr칤tico".
* **Fortalezas:** Capaz de describir detalles min칰sculos en im치genes, realizar OCR de alta precisi칩n y entender diagramas complejos.
* **Costo de Hardware:** Muy Alto. Durante su uso, la VRAM estar치 al l칤mite. Se recomienda no realizar tareas de imagen simult치neas.

---

## 丘뙖잺 Arquitectura del Servidor (v6.8)

El servidor en el puerto **8009** gestiona la inteligencia ling칲칤stica:

1. **Dualidad Chat/Vision:** Detecta autom치ticamente si el request incluye im치genes para invocar el motor VLM.
2. **Accounting:** Registra cada inferencia en `/logs/text/inference.log` con duraci칩n y conteo de tokens.
3. **Serializaci칩n:** Al igual que el de imagen, procesa 1 a 1 para proteger la CPU del MacBook.

---

## 九꽲잺 Prompt Engineering Ling칲칤stico

1. **System Prompts:** El servidor soporta prompts de sistema para definir personalidad.
2. **Visi칩n:** Para mejores resultados en VLM, indicar siempre la ruta absoluta del archivo y una pregunta espec칤fica (ej: "쯈u칠 texto hay en el cartel?").

---
**Nota:** El uso del modelo de 32B puede disparar alertas en el `monitor_resources.sh`. Monitorear la presi칩n t칠rmica es obligatorio en sesiones largas.
